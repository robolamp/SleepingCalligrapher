{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport re\nimport time\n# import datetime\nfrom datetime import datetime, timedelta\n\n\n%matplotlib inline","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### Reading initial dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/kuzushiji-recognition/train.csv')\nunicode_map = {codepoint: char for codepoint, char in \n               pd.read_csv('../input/kuzushiji-recognition/unicode_translation.csv').values}","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A bit of statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_labels_set(labels_str):\n    labels = []\n    for one_label_str in re.findall(r'U\\+\\S+\\s\\S+\\s\\S+\\s\\S+\\s\\S+', labels_str):\n        charcode, x, y, w, h = one_label_str.split(' ')\n        labels.append([charcode, int(x), int(y), int(w), int(h)])\n    return labels","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_labels = 0\nchars_counts = {}\n\nfor labels_set in df_train.values[:, 1]:\n    if type(labels_set) is not str:\n        continue\n\n    labels = convert_labels_set(labels_set)\n    n_labels += len(labels)\n    for label in labels:\n        try:\n            chars_counts[label[0]] += 1\n        except KeyError:\n            chars_counts.update({label[0]: 1})\n\nchars_counts_list = [chars_counts[k] for k in chars_counts]\nn_classes = len([k for k in chars_counts])\n\nprint('Number of labels:                  {}'.format(n_labels))\nprint('Number of classes:                 {}'.format(n_classes))\nprint('Min max number of items per class: {} {}'.format(np.min(chars_counts_list), np.max(chars_counts_list)))\nprint('Median number of items per class:  {}'.format(np.median(chars_counts_list)))\nprint('Mean number of items per class:    {}'.format(np.mean(chars_counts_list)))","execution_count":4,"outputs":[{"output_type":"stream","text":"Number of labels:                  683464\nNumber of classes:                 4212\nMin max number of items per class: 1 24685\nMedian number of items per class:  9.0\nMean number of items per class:    162.26590693257359\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Making dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_char_images_from_sheet(src_image_path, labels_str, blur_kernel_size=3, img_size=64):\n    src_img = cv2.imread(src_image_path, cv2.IMREAD_COLOR)\n\n    char_imgs, labels_codes = [], []\n    for label in convert_labels_set(labels_str):\n        char_img = np.zeros((img_size, img_size), dtype=np.uint8)\n        charcode, x, y, w, h = label\n\n        label_img = src_img[y:y + h, x:x + w, :]\n        label_img = cv2.GaussianBlur(label_img, \n                                     (blur_kernel_size, blur_kernel_size), \n                                     cv2.BORDER_DEFAULT)\n        label_img = cv2.cvtColor(label_img, cv2.COLOR_RGB2GRAY)\n        _, label_img = cv2.threshold(label_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n        label_img = 255 - label_img\n        \n        if w > h:\n            label_img = cv2.resize(label_img, (img_size, int(img_size * h / w)))\n            dy = int((img_size - int(img_size * h / w)) / 2)\n            char_img[dy:dy + int(img_size * h / w), :] += label_img\n        \n        else:\n            label_img = cv2.resize(label_img, (int(img_size * w / h), img_size))            \n            dx = int((img_size - int(img_size * w / h)) / 2)\n            char_img[:, dx:dx + int(img_size * w / h)] += label_img\n        \n        char_imgs.append(char_img)\n        labels_codes.append(charcode)\n    return char_imgs, labels_codes","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_size = 64\n\nchars_imgs = []\nchars_labels = []\n\nfor value in df_train.values:\n    img_filename, labels_str = value\n    try:\n        imgs, labels = get_char_images_from_sheet('../input/kuzushiji-recognition/train_images/{}.jpg'.format(img_filename), labels_str, img_size=img_size)\n    except TypeError:\n#         print('Unable to parse {}'.format(labels_str))\n        continue\n#     imgs = [(image - 127.5).astype('float32') / 127.5 for image in imgs]\n    imgs = [image.reshape(img_size, img_size, 1) for image in imgs]\n    chars_imgs.extend(imgs)\n    chars_labels.extend(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chars_imgs = np.array(chars_imgs)\nnp.min(chars_imgs), np.max(chars_imgs), chars_imgs.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DCGAN: The simplest example"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import layers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((img_size / 4, img_size / 4, 256)))\n    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    assert model.output_shape == (None, img_size / 4, img_size / 4, 128)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, img_size / 2, img_size / 2, 64)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    assert model.output_shape == (None, img_size, img_size, 1)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = make_generator_model()\n\nnoise = tf.random.normal([1, 100])\ngenerated_image = generator(noise, training=False)\n\nplt.imshow(generated_image[0, :, :, 0], cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Discriminator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n                                     input_shape=[img_size, img_size, 1]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator = make_discriminator_model()\ndecision = discriminator(generated_image)\nprint (decision)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Losses"},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Optimisers"},{"metadata":{"trusted":true},"cell_type":"code","source":"generator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Training loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 10\nBUFFER_SIZE = 60000\nBATCH_SIZE = 256\n\nnoise_dim = 100\nnum_examples_to_generate = 16\n\n# We will reuse this seed overtime (so it's easier)\n# to visualize progress in the animated GIF)\nseed = tf.random.normal([num_examples_to_generate, noise_dim])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_and_save_images(model, epoch, test_input):\n  # Notice `training` is set to False.\n  # This is so all layers run in inference mode (batchnorm).\n  predictions = model(test_input, training=False)\n\n  fig = plt.figure(figsize=(4,4))\n\n  for i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n      plt.axis('off')\n\n  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n  plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}